---
sidebar: sidebar
permalink: use/maintain-cluster.html
keywords: astra, astra data store, kubectl
summary: You can maintain the cluster by using kubectl commands with Astra Data Store.
---

= Manage the Astra Data Store cluster
:hardbreaks:
:icons: font
:imagesdir: ../media/use/

You can manage the cluster by using kubectl commands with Astra Data Store.

* <<Add a node>>
* <<Remove a node>>
* <<Place a node in maintenance mode>>
* <<Add drives to a node>>
//* <<Replace a node>>
* <<Replace a drive>>


.What you'll need

* System with kubectl and kubectl-astrads plugin installed. See link:../get-started/install-ads.html[Install Astra Data Store].

== Add a node

The node that you are adding should be part of the Kubernetes cluster and should have a configuration that is similar to the other nodes in the cluster.

NOTE: To add a node using Astra Control Center, see https://docs.netapp.com/us-en/astra-control-center/use/manage-backend.html[Add nodes to a storage backend cluster^].

.Steps
.	If the new node’s dataIP is not already part of the Astra Data Store cluster CR, do the following:
.. Edit the cluster CR and add the additional dataIP in the `adsDataNetworks` *addresses* field. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl edit astradscluster CLUSTER_NAME -n astrads-system
----
+
Response:
+
----
adsDataNetworks:
    -addresses:  dataIP1,dataIP2,dataIP3,dataIP4,NEW_DATA_IP
----

.. Save the CR.
.. Add the node to the Astra Data Store cluster. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads nodes add --cluster CLUSTER_NAME
----

.	Otherwise, just add the nodes. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads nodes add --cluster CLUSTER_NAME
----

. Verify that the node has been added:
+
[source,kubectl]
----
kubectl astrads nodes list
----

== Remove a node
Use kubectl commands with Astra Data Store to remove a node in a cluster.

.Steps

. List all the nodes:
+
[source,kubectl]
----
kubectl astrads nodes list
----
+
Response:
+
----
NODE NAME           NODE STATUS    CLUSTER NAME
sti-rx2540-534d...  Added       cluster-multinodes-21209
sti-rx2540-535d...  Added       cluster-multinodes-21209
...
----
. Mark the node for removal. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads nodes remove NODE_NAME
----
+
Response:
+
----
Removal label set on node sti-rx2540-534d.lab.org
Successfully updated ADS cluster cluster-multinodes-21209 desired node count from 4 to 3
----
+
After you mark the node for removal, the node status should change from `active` to `present`.
. Verify the `present` status of the removed node:
+
[source,kubectl]
----
kubectl get nodes --show-labels
----
+
Response:
+
----
NAME                                            STATUS   ROLES                  AGE     VERSION   LABELS
sti-astramaster-050.lab.org   Ready    control-plane,master   3h39m   v1.20.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sti-astramaster-050.lab.org,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
sti-rx2540-556a.lab.org       Ready    worker                 3h38m   v1.20.0   astrads.netapp.io/cluster=astrads-cluster-890c32c,astrads.netapp.io/storage-cluster-status=active,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sti-rx2540-556a.lab.org,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true
sti-rx2540-556b.lab.org       Ready    worker                 3h38m   v1.20.0   astrads.netapp.io/cluster=astrads-cluster-890c32c,astrads.netapp.io/storage-cluster-status=active,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sti-rx2540-556b.lab.org,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true
sti-rx2540-534d.lab.org       Ready    worker                 3h38m   v1.20.0   astrads.netapp.io/storage-cluster-status=present,astrads.netapp.io/storage-node-removal=,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sti-rx2540-557a.lab.org,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true
sti-rx2540-557b.lab.org       Ready    worker                 3h38m   v1.20.0   astrads.netapp.io/cluster=astrads-cluster-890c32c,astrads.netapp.io/storage-cluster-status=active,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sti-rx2540-557b.lab.org,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true
----
. Uninstall Astra Data Store from the node. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads nodes uninstall NODE_NAME
----
. Verify the node is removed from the cluster:
+
[source,kubectl]
----
kubectl astrads nodes list
----

.Result
The node is removed from Astra Data Store.


== Place a node in maintenance mode

When you need to perform host maintenance or package upgrades, you should place the node in maintenance mode.

NOTE: The node must already be part of the Astra Data Store cluster.

When a node is in maintenance mode, you cannot add a node to the cluster. In this example, we will place node `nhcitjj1525` into maintenance mode.

.Steps

. Display the node details:
+
[source,kubectl]
----
kubectl get nodes
----
+
Response:
+
----
 NAME             STATUS   ROLES                  AGE     VERSION
 nhcitjj1525      Ready    <none>                 3d18h   v1.20.0
 nhcitjj1526      Ready    <none>                 3d18h   v1.20.0
 nhcitjj1527      Ready    <none>                 3d18h   v1.20.0
 nhcitjj1528      Ready    <none>                 3d18h   v1.20.0
 scs000039783-1   Ready    control-plane,master   3d18h   v1.20.0
----

. Ensure that the node is not already in maintenance mode:
+
[source,kubectl]
----
kubectl astrads maintenance list
----
+
Response (there are no nodes already in maintenance mode):
+
----
NAME    NODE NAME  IN MAINTENANCE  MAINTENANCE STATE       MAINTENANCE VARIANT
----

. Enable maintenance mode. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads maintenance create CR_NAME --node-name=NODE_NAME --variant=Node
----
+
For example:
+
[source,kubectl]
----
kubectl astrads maintenance create maint1 --node-name="nhcitjj1525" --variant=Node
----
+
Response:
+
----
Maintenance mode astrads-system/maint1 created
----
. List the nodes:
+
[source,kubectl]
----
kubectl astrads nodes list
----
+
Response:
+
----
NODE NAME       NODE STATUS     CLUSTER NAME
nhcitjj1525     Added           ftap-astra-012
...
----

. Check the status of the maintenance mode:
+
[source,kubectl]
----
kubectl astrads maintenance list
----
+
Response:
+
----
NAME    NODE NAME       IN MAINTENANCE  MAINTENANCE STATE       MAINTENANCE VARIANT
node4   nhcitjj1525     true            ReadyForMaintenance     Node
----
+
The `In Maintenance` mode starts as `false` and changes to `true`.
The `Maintenance State` changes from `PreparingForMaintenance` to `ReadyforMaintenance`.

. After the node maintenance is complete, disable maintenance mode:
+
[source,kubectl]
----
kubectl astrads maintenance update maint1 --node-name="nhcitjj1525" --variant=None
----

. Ensure that the node is no longer in maintenance mode:
+
[source,kubectl]
----
kubectl astrads maintenance list
----

== Add drives to a node

Use kubectl commands with Astra Data Store to add physical or virtual drives to a node in an Astra Data Store cluster.

.What you'll need

* One or more drives that meet the following criteria:
** Already installed in the node (physical drives) or added to the node VM (virtual drives)
** No partitions on the drive
** Drive is not currently in use by the cluster
** Drive raw capacity does not exceed licensed raw capacity in the cluster (For example, with a license granting 2TB of storage per CPU core, a cluster of 10 nodes would have a maximum of 20TB raw drive capacity)
** Drive is at least the size of other active drives in the node

NOTE: Astra Data Store requires no more than 16 drives per node. If you try to add a 17th drive, the drive add request is denied.

.Steps

. Describe the cluster:
+
[source,kubectl]
----
kubectl astrads clusters list
----
+
Response:
+
----
CLUSTER NAME                    CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209        created         4
----
. Note the cluster name.
. Show the drives that are available to add to all nodes in the cluster. Replace CLUSTER_NAME with the name of your cluster:
+
[source,kubectl]
----
kubectl astrads drives adddrive show-available --cluster=CLUSTER_NAME
----
+
Response:
+
----
Node: node1.name
Add drive maximum size: 100.0 GiB
Add drive minimum size: 100.0 GiB
NAME IDPATH SERIAL PARTITIONCOUNT SIZE ALREADYINCLUSTER
sdg /dev/disk/by-id/scsi-3c290e16d52479a9af5eac c290e16d52479a9af5eac 0 100 GiB false
sdh /dev/disk/by-id/scsi-3c2935798df68355dee0be c2935798df68355dee0be 0 100 GiB false

Node: node2.name
Add drive maximum size: 66.7 GiB
Add drive minimum size: 100.0 GiB
No suitable drives to add exist.

Node: node3.name
Add drive maximum size: 100.0 GiB
Add drive minimum size: 100.0 GiB
NAME IDPATH SERIAL PARTITIONCOUNT SIZE ALREADYINCLUSTER
sdg /dev/disk/by-id/scsi-3c29ee82992ed7a36fc942 c29ee82992ed7a36fc942 0 100 GiB false
sdh /dev/disk/by-id/scsi-3c29312aa362469fb3da9c c29312aa362469fb3da9c 0 100 GiB false

Node: node4.name
Add drive maximum size: 66.7 GiB
Add drive minimum size: 100.0 GiB
No suitable drives to add exist.
----
. Do one of the following:
* If all available drives have the same name, you can add them to the respective nodes simultaneously. Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads drives adddrive create --cluster=CLUSTER_NAME --name REQUEST_NAME --drivesbyname all=DRIVE_NAME
----
* If the drives are named differently, you can add them to the respective nodes one at a time (you'll need to repeat this step for each drive you need to add). Replace the information in capital letters with the appropriate values for your environment:
+
[source,kubectl]
----
kubectl astrads drives adddrive create --cluster=CLUSTER_NAME --name REQUEST_NAME --drivesbyname NODE_NAME=DRIVE_NAME
----

.Result
Astra Data Store creates a request to add the drive or drives, and a message appears with the result of the request.


////
== Replace a node

Use kubectl commands with Astra Data Store to replace a failed node in a cluster.

.Steps

. Remove the node from Astra Data Store.  See <<Remove a node>>.

. Add a node to the cluster for replacement by using a text editor to modify the cluster CR. In this example, the node count increments to 4:
+
----
rvi manifests/astradscluster.yaml
cat manifests/astradscluster.yaml
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  name: cluster-multinodes-21209
  namespace: astrads-system
spec:
...
  # Specify the number of nodes that should be used for creating ADS cluster
  adsNodeCount: 4
...
----
. Apply the modified cluster CR:
+
[source,kubectl]
----
kubectl apply -f manifests/astradscluster.yaml
----
+
Response:
+
----
astradscluster.astrads.netapp.io/cluster-multinodes-21209 configured
----
. Verify that new node is picked up for addition:
+
[source,kubectl]
----
kubectl astrads nodes list
----
+
Response:
+
----
NODE NAME                NODE STATUS     CLUSTER NAME
sti-rx2540-534d...       Added           cluster-multinodes-21209
sti-rx2540-535d...       Added           cluster-multinodes-21209
----


+
[source,sh]
----
kubectl get pods -n astrads-system
----
+
Response:
+
----
NAME                                READY   STATUS    RESTARTS   AGE
astrads-cluster-controller...       1/1     Running   1          24h
astrads-deployment-support...       3/3     Running   0          24h
astrads-ds-cluster-multinodes-21209 1/1     Running
----
+
[source,sh]
----
kubectl astrads clusters list
----
+
Response:
+
----
CLUSTER NAME                    CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209        created         4
----
+
[source,sh]
----
kubectl astrads drives list
----
+
Response:
+
----
DRIVE NAME    DRIVE ID    DRIVE STATUS   NODE NAME     CLUSTER NAME
scsi-36000..  c3e197f2... Active         sti-rx2540... cluster-multinodes-21209
----

. List all the nodes:
+
[source,sh]
----
kubectl astrads nodes list
----
+
Response:
+
----
NODE NAME           NODE STATUS    CLUSTER NAME
sti-rx2540-534d..   Added       cluster-multinodes-21209
sti-rx2540-535d...  Added       cluster-multinodes-21209
...
----

. Describe the cluster:
+
[source,sh]
----
kubectl astrads clusters list
----
+
Response:
+
----
CLUSTER NAME               CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209   created         4
----

. Verify that `Node HA` is marked as `false` on the failed node:
+
[source,sh]
----
kubectl describe astradscluster -n astrads-system
----
+
Response:
+
----
Name:         cluster-multinodes-21209
Namespace:    astrads-system
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"astrads.netapp.io/v1alpha1","kind":"AstraDSCluster","metadata":{"annotations":{},"name":"cluster-multinodes-21209","namespa...
API Version:  astrads.netapp.io/v1alpha1
Kind:         AstraDSCluster

State:               Disabled
Variant:             None
Node HA:             false
Node ID:             4
Node Is Reachable:   false
Node Management IP:  172.21.192.192
Node Name:           sti-rx2540-532d.ctl.gdl.englab.netapp.com
Node Role:           Storage
Node UUID:           6f6b88f3-8411-56e5-b1f0-a8e8d0c946db
Node Version:        12.75.0.6167444
Status:              Added
----

. Modify the astradscluster CR to remove the failed node by decrementing the value of `AdsNode Count' to 3:
+
[source,sh]
----
cat manifests/astradscluster.yaml
----
+
Response:
+
----
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  name: cluster-multinodes-21209
  namespace: astrads-system
spec:
  # ADS Node Configuration per node settings
  adsNodeConfig:
    # Specify CPU limit for ADS components
    # Supported value: 9
    cpu: 9
    # Specify Memory Limit in GiB for ADS Components.
    # Your kubernetes worker nodes need to have at least this much RAM free
    # for ADS to function correctly
    # Supported value: 34
    memory: 34
    # [Optional] Specify raw storage consumption limit. The operator will only select drives for a node up to this limit
    capacity: 600
    # [Optional] Set a cache device if you do not want auto detection e.g. /dev/sdb
    # cacheDevice: ""
    # Set this regex filter to select drives for ADS cluster
    # drivesFilter: ".*"

  # [Optional] Specify node selector labels to select the nodes for creating ADS cluster
  # adsNodeSelector:
  #   matchLabels:
  #     customLabelKey: customLabelValue

  # Specify the number of nodes that should be used for creating ADS cluster
  adsNodeCount: 3

  # Specify the IP address of a floating management IP routable from any worker node in the cluster
  mvip: "172..."

  # Comma separated list of floating IP addresses routable from any host where you intend to mount a NetApp Volume
  # at least one per node must be specified
  # addresses: 10.0.0.1,10.0.0.2,10.0.0.3,10.0.0.4,10.0.0.5
  # netmask: 255.255.255.0
  adsDataNetworks:
    - addresses: "172..."
      netmask: 255.255.252.0


  # [Optional] Provide a k8s label key that defines which protection domain a node belongs to
  # adsProtectionDomainKey: ""

  # [Optional] Provide a monitoring config to be used to setup/configure a monitoring agent.
  monitoringConfig:
   namespace: "netapp-monitoring"
   repo: "docker.repo.eng.netapp.com/global/astra"

  autoSupportConfig:
    # AutoUpload defines the flag to enable or disable AutoSupport upload in the cluster (true/false)
    autoUpload: true
    # Enabled defines the flag to enable or disable automatic AutoSupport collection.
    # When set to false, periodic and event driven AutoSupport collection would be disabled.
    # It is still possible to trigger an AutoSupport manually while AutoSupport is disabled
    # enabled: true
    # CoredumpUpload defines the flag to enable or disable the upload of coredumps for this ADS Cluster
    # coredumpUpload: false
    # HistoryRetentionCount defines the number of local (not uploaded) AutoSupport Custom Resources to retain in the cluster before deletion
    historyRetentionCount: 25
    # DestinationURL defines the endpoint to transfer the AutoSupport bundle collection
    destinationURL: "https://testbed.netapp.com/put/AsupPut"
    # ProxyURL defines the URL of the proxy with port to be used for AutoSupport bundle transfer
    # proxyURL:
    # Periodic defines the config for periodic/scheduled AutoSupport objects
    periodic:
      # Schedule defines the Kubernetes Cronjob schedule
      - schedule: "0 0 * * *"
        # PeriodicConfig defines the fields needed to create the Periodic AutoSupports
        periodicconfig:
        - component:
            name: storage
            event: dailyMonitoring
          userMessage: Daily Monitoring Storage AutoSupport bundle
          nodes: all
        - component:
            name: controlplane
            event: daily
          userMessage: Daily Control Plane AutoSupport bundle
----

. Verify the node is removed from the cluster:
+
[source,sh]
----
kubectl get nodes --show-labels
----
+
Response:
+
----

NAME                  STATUS   ROLES               AGE   VERSION   LABELS
sti-astramaster-237   Ready control-plane,master   24h   v1.20.0
sti-rx2540-532d       Ready  <none>                24h   v1.20.0
sti-rx2540-533d       Ready  <none>                24h
----
+
[source,sh]
----
kubectl astrads nodes list
----
+
Response:
+
----
NODE NAME         NODE STATUS     CLUSTER NAME
sti-rx2540-534d   Added           cluster-multinodes-21209
sti-rx2540-535d   Added           cluster-multinodes-21209
sti-rx2540-536d   Added           cluster-multinodes-21209
----
+
[source,sh]
----
kubectl get nodes --show-labels
----
+
Response:
+
----
NAME                STATUS   ROLES                  AGE   VERSION   LABELS
sti-astramaster-237 Ready    control-plane,master   24h   v1.20.0   beta.kubernetes.io/arch=amd64,
sti-rx2540-532d     Ready    <none>                 24h   v1.20.0   astrads.netapp.io/node-removal
----
+
[source,sh]
----
kubectl describe astradscluster -n astrads-system
----
+
Response:
+
----
Name:         cluster-multinodes-21209
Namespace:    astrads-system
Labels:       <none>
Kind:         AstraDSCluster
Metadata:
...
----

////

== Replace a drive

When a drive fails in a cluster, the drive must be replaced as soon as possible to ensure data integrity.
If a drive fails, you can see information about the failed drive in cluster CR node status, cluster health condition information, and the metrics endpoint. You can use the following example commands to see failed drive information.

.Example of cluster showing failed drive in nodeStatuses.driveStatuses

[source,kubectl]
----
kubectl get adscl -A -o yaml
----
Response:
----
...
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
...
nodeStatuses:
  - driveStatuses:
    - driveID: 31205e51-f592-59e3-b6ec-185fd25888fa
      driveName: scsi-36000c290ace209465271ed6b8589b494
      drivesStatus: Failed
    - driveID: 3b515b09-3e95-5d25-a583-bee531ff3f31
      driveName: scsi-36000c290ef2632627cb167a03b431a5f
      drivesStatus: Active
    - driveID: 0807fa06-35ce-5a46-9c25-f1669def8c8e
      driveName: scsi-36000c292c8fc037c9f7e97a49e3e2708
      drivesStatus: Active
...
----

.Example of new AstraDSFailedDrive CR

The failed drive CR is created automatically in the cluster with a name corresponding to the UUID of the failed drive.

[source,kubectl]
----
kubectl get adsfd -A -o yaml
----
Response:
----
...
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSFailedDrive
metadata:
    name: c290a-5000-4652c-9b494
    namespace: astrads-system
spec:
  executeReplace: false
  replaceWith: ""
 status:
   cluster: arda-6e4b4af
   failedDriveInfo:
     failureReason: AdminFailed
     inUse: false
     name: scsi-36000c290ace209465271ed6b8589b494
     path: /dev/disk/by-id/scsi-36000c290ace209465271ed6b8589b494
     present: true
     serial: 6000c290ace209465271ed6b8589b494
     node: sti-rx2540-300b.lab.org
   state: ReadyToReplace
----

[source,kubectl]
----
kubectl astrads faileddrive list --cluster arda-6e4b4af
----

Response:
----
NAME       NODE                             CLUSTER        STATE                AGE
6000c290   sti-rx2540-300b.lab.netapp.com   ard-6e4b4af    ReadyToReplace       13m
----

.Steps

. List possible replacement drives with the `kubectl astrads faileddrive show-replacements` command, which filters drives that fit replacement restrictions (unused in cluster, not mounted, no partitions, and equal or larger than failed drive).
+
To list all drives without filtering possible replacement drives, add `--all` to `show-replacements` command.
+
[source,kubectl]
----
kubectl astrads faileddrive show-replacements --cluster ard-6e4b4af --name 6000c290
----
+
Response:
+
----
NAME  IDPATH             SERIAL  PARTITIONCOUNT   MOUNTED   SIZE
sdh   /scsi-36000c29417  45000c  0                false     100GB
----

. Use the `replace` command to replace the drive with the passed serial number. The command completes the replacement or fails if `--wait` time elapses.
+
[source,kubectl]
----
kubectl astrads faileddrive replace --cluster arda-6e4b4af --name 6000c290 --replaceWith 45000c --wait
Drive replacement completed successfully
----
+
NOTE: If `kubectl astrads faileddrive replace` is executed using an inappropriate `--replaceWith` serial number, an error appears similar to this:
+
[source,kubectl]
----
kubectl astrads replacedrive replace --cluster astrads-cluster-f51b10a --name 6000c2927 --replaceWith BAD_SERIAL_NUMBER
Drive 6000c2927 replacement started
Failed drive 6000c2927 has been set to use BAD_SERIAL_NUMBER as a replacement
...
Drive replacement didn't complete within 25 seconds
Current status: {FailedDriveInfo:{InUse:false Present:true Name:scsi-36000c2 FiretapUUID:444a5468 Serial:6000c Path:/scsi-36000c FailureReason:AdminFailed Node:sti-b200-0214a.lab.netapp.com} Cluster:astrads-cluster-f51b10a State:ReadyToReplace Conditions:[{Message: "Replacement drive serial specified doesn't exist", Reason: "DriveSelectionFailed", Status: False, Type:' Done"]}
----


. To re-run drive replacement use `--force` with the previous command:

+
[source,kubectl]
----
kubectl astrads replacedrive replace --cluster astrads-cluster-f51b10a --name 6000c2927 --replaceWith VALID_SERIAL_NUMBER --force
----

== For more information

* link:../use/kubectl-commands-ads.html[Manage Astra Data Store resources with kubectl commands]
* https://docs.netapp.com/us-en/astra-control-center/use/manage-backend.html#add-nodes-to-a-storage-backend-cluster[Add nodes to a storage backend cluster in Astra Control Center^]
*
