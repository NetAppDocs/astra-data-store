---
sidebar: sidebar
permalink: use/kubectl-commands-ads.html
keywords: astra, astra data store, astra datastore, kubectl
summary: You can use kubectl commands directly with Astra Data Store.
---

= Manage Astra Data Store with kubectl commands
:hardbreaks:
:icons: font
:imagesdir: ../media/get-started/

You can manage Astra Data Store assets by using kubectl commands.

These instructions show you how to manage Astra Data Store assets by using a sample MongoDB app and YSCB IO Tool workflow. The sample app provides context for the examples. The application workload structure includes one MongoDB deployment consisting of a replica-set, which creates a mongo pod and with a MongoDB container running inside that pod.

The deployment references a Trident persistent volume claim (PVC). This PVC initializes its own PV: `astradsvolume` and `astradsexportpolicy`. When the YSCB tool is started, IO is sent to the Mongo container inside the pod, and the mounted `astradsvolume` is filled with data.

While all artifacts in these instructions exist in the same namespaces (for example: `astrads-system`), you can instead have their internal namespaces assigned to separate apps from backend artifacts.

== Some tasks you can do with kubectl commands

You can perform many Astra Data Store tasks with kubectl commands.

Here are instructions for some of them:

* <<Describe the deployment>>
* <<Display volumes>>
* <<Display the server IP>>
* <<Describe the volume>>
* <<Show the volume mount on a node>>
* <<Show the volume mount inside a MongoDB container>>
* <<Place a node in maintenance mode>>
* <<Replace a drive>>
* <<Replace a node>>


== Available commands

* *asup*: Manage AutoSupport.
* *clusters*: Display a cluster.
* *drives*: List drives in a cluster â€“ just lists drives
* *faileddrive*: Replace a failed drive in a cluster.
* *help*: Get help about any command.
* *license*: Add a license in the astrads cluster, during the initial Astra Data Store deployment or if you need to update a license after deployment.
* *maintenance*: Place a node into maintenance mode.
* *monitoring*: Manage monitoring output by deploying the monitoring operator. This is done during the initial Astra Data Store deployment.
+
See link:../get-started/install-ads.html#install-the-monitoring-operator[Configure Astra Data Store for monitoring].
* *nodes*: List the nodes in a cluster.

== List of kubectl API resources
You can see a list of kubectl API resources as shown in the sample below.

====
 kubectl api-resources --api-group astrads.netapp.io
NAME                   SHORTNAMES  APIGROUP           NAMESPACED  KIND
astradsversions         adsve      astrads.netapp.io  true        AstraDSVersion
astradsclusters         adscl      astrads.netapp.io  true        AstraDSCluster
astradslicenses         adsli      astrads.netapp.io  true        AstraDSLicense
astradsnodeinfoes       adsni      astrads.netapp.io  true        AstraDSNodeInfo
astradsvolumes          adsvo      astrads.netapp.io  true        AstraDSVolume
astradsqospolicies      adsqp      astrads.netapp.io  true        AstraDSQosPolicy
astradsexportpolicies   adsep      astrads.netapp.io  true        AstraDSExportPolicy
astradsvolumesnapshots  adsvs      astrads.netapp.io  true        AstraDSVolumeSnapshot
astradsvolumefiles      adsvf      astrads.netapp.io  true        AstraDSVolumeFiles
astradsautosupports     adsas      astrads.netapp.io  true        AstraDSAutoSupport
astradsfaileddrives     adsfd      astrads.netapp.io  true        AstraDSFailedDrive
astradsnodemanagements  adsnm      astrads.netapp.io  true        AstraDSNodeManagement
====



== Describe the deployment

Enter: `kubectl describe deployment -n astrads-system mongo0`

Response:

====
 Pod template:
  Labels: app=mongo0
          ...
          role=database
  Containers:
    mongo0:
      Image: mongo0
      Post: 27017/TCP
      Host Port:
      Command:
        Mongod
         --bind_ip_all
      Environment: <none>
      Mounts:
        /data/db from target10 (rw)
  Volumes:
    target10:
      Type:
      ClaimName: mongo-pvc0
      ReadOnly: false
====


== Display volumes

Display the volumes to which it is bound.

Enter: `kubectl describe pvc -n astrads-system mongo0`

Response:
====
 Name:         mongo-pvc0
 Namespace:    astrads-system
 StorageClass: trident-csi
 Status:       Bound
 Volume:
====

== Display the server IP

Enter: `kubectl get astradsvolumes -n astrads-system | grep <volume_ID>``

Response:

====
 <volume_ID> 8Gi astrads-cluster-420a40  true
====



== Describe the volume

Enter: `kubectl describe astradsvolumes -n astrads-system <volume_ID>``

Response:

====
 Spec:
  Cluster:
  Display Name:
  Export Policy:
  No Snap Dir: true
  Permissions:
  QoS Policy: Read/Write
  Volume Path:
Status:
  Cluster:
  Conditions:
    Last Transmission Time:
    Message:
    Reason: VolumeOnline
    Status: True
    Type: AstraDSVolumeOnline
====

== Show the volume mount on a node

. Enter: `kubectl get pods -n astrads-system`

+
Response:
+
====
 mongo00  1/1 Running 0   23m
 mongo01  1/1 Running 0   21m
====

. Enter: `kubectl describe pod -n astrads-system mongo0  | grep Node`

+
Response:
+
====
 Node:
 Node-Selectors: <none>
====

. Enter: `ssh <IP-address> -l root`
. Enter: `mount | grep pvc`

== Show the volume mount inside a MongoDB container

. Enter: `kubectl get pods -n astrads-system`

+
Response:
+
====
 mongo0  1/1 Running 0   23m
 mongo1  1/1 Running 0   21m
====

. Enter: `kubectl exec -it -n astrads-system mongo0 <pod_id>`
. Log into MongoDB.
. Enter: `show dbs`
. Enter: `use ycsb-_rand0_`
. Enter: `show collections`

== Place a node in maintenance mode

When you need to perform host maintenance or package upgrades, you should place the node in maintenance mode.

NOTE: The node must be part of the Astra Data Store cluster.

When a node is in maintenance mode, you cannot add a node.

.Steps

. Display the node details.
+
====
 kubectl get nodes
====

. Get details of volumes:
+
====
 kubectl get astradsvolumes -n astrads-system -o wide
NAME      SIZE  IP              CLUSTER          EXPORTPOLICY     CREATED
nfsvol1   102Gi 10.111.111.111  ftap-astra-012   exppol1    true
nfsvol10  102Gi 10.111.111.112  ftap-astra-012   exppol10   true
nfsvol11  102Gi 10.111.111.113  ftap-astra-012   exppol11   true
====

. Enable maintenance mode:
+
====
 kubectl astrads maintenance list
NAME    NODE NAME       IN MAINTENANCE  MAINTENANCE STATE       MAINTENANCE VARIANT

 kubectl astrads maintenance create node4 --node-name="nhcitjj1525" --variant=Node
Maintenance mode astrads-system/node4 created

 kubectl astrads nodes list
NODE NAME       NODE STATUS     CLUSTER NAME
nhcitjj1525     Added           ftap-astra-012
nhcitjj1527     Added           ftap-astra-012
nhcitjj1526     Added           ftap-astra-012
nhcitjj1528     Added           ftap-astra-012
...

 kubectl astrads maintenance list
NAME    NODE NAME       IN MAINTENANCE  MAINTENANCE STATE       MAINTENANCE VARIANT
node4   nhcitjj1525     true            ReadyForMaintenance     Node
====

+
The In Maintenance mode starts as "False" and changes to "True" and the Maintenance state changes from "Preparing for Maintenance" to "Ready for Maintenance."


== Replace a drive

The following provides an overview of the steps needed to replace a failed drive in a cluster.

* ssh into the nodes
** Get a list of active drives.
** Get a list of all the drives linked to the node.
* Identify available drives.
* Get notice of a failed drive Custom Resource (CR).
* Replace the drive.
* Validate if the drive is successfully replaced and active in the cluster CR.

.Steps
. Get the cluster details:
+
====
 kubectl describe <cluster_ID> -n
====

+
Sample:
+
====
 kubectl describe adscl -n astrads-system
====
. ssh into the nodes:
+
====
 ssh root@<ip_address>
====

. List all the active drives on that node:
+
====
 runc exec -t firetap /sf/packages/netapp-photon/cmd_firestorm.py -c 'disk show'
====
+
Response:
+
====
 DISK   OWNER        POOL   SERIAL  HOME        DR HOME
 -----  ----------   -----  ------  ---------   -------
 v0.0   fires-9(09)  Pool0  1234   fires-9(09)
 v0.1   fires-9(09)  Pool0  5678   fires-9(09)
 v0.2   fires-9(09)  Pool0  9101   fires-9(09)
 v0.3   fires-9(09)  Pool0  1213   fires-9(09)
====

. List all the drives on that node:
+
====
 lsblk -o NAME,SERIAL,SIZE
====


. Create a failed drive CR:
+
====
 kubectl get adsfd -n astrads-system
====
+
Response:
+
====
 NAME                                   AGE
 158c66c5-3e84-5530-8ede-d8e3cbbf67af   37s
====

. Get the failed drive details:
+
====
 Name:         158c66c5-3e84-5530-8ede-d8e3cbbf67af
Namespace:    astrads-system
Labels:       <none>
Annotations:  <none>
API Version:  astrads.netapp.io/v1alpha1
Kind:         AstraDSFailedDrive
Metadata:
 Creation Timestamp:  2021-10-26T06:36:12Z
 Generation:          1
 Managed Fields:
   API Version:  astrads.netapp.io/v1alpha1
   Fields Type:  FieldsV1
   fieldsV1:
     f:spec:
       .:
       f:executeReplace:
       f:replaceWith:
     f:status:
       .:
       f:cluster:
       f:failedDriveInfo:
         .:
         f:failureReason:
         f:firetapUUID:
         f:inUse:
         f:name:
         f:node:
         f:path:
         f:present:
         f:serial:
         f:sizeBytes:
       f:state:
   Manager:         cluster-controller
   Operation:       Update
   Time:            2021-10-26T06:36:12Z
 Resource Version:  4110227
 UID:               14a2c23b-fcd8-4b04-ae25-48c75abc0682
Spec:
 Execute Replace:  false
 Replace With:
Status:
 Cluster:  astrads-cluster-493a7f8
 Failed Drive Info:
   Failure Reason:  AdminFailed
   Firetap UUID:    158c66c5-3e84-5530-8ede-d8e3cbbf67af
   In Use:          false
   Name:            scsi-36000c29abd71fd0dad31270af16bb1bc
   Node:            sti-b200-0214b.ctl.gdl.englab.netapp.com
   Path:            /dev/disk/by-id/scsi-36000c29abd71fd0dad31270af16bb1bc
   Present:         false
   Serial:          6000c29abd71fd0dad31270af16bb1bc
   Size Bytes:      107374182400
 State:             ReadyToReplace
Events:              <none>
====

. Edit the failed drive CR and replace it with available drive.
+
====
 kubectl edit adsfd -n astrads-system
====
+
Response:
+
====
 astradsfaileddrive.astrads.netapp.io/158c66c5-3e84-5530-8ede-d8e3cbbf67af edited
...
Spec:
  Execute Replace:  true
  Replace With:     6000c2949046697ae1c738208ffc6620
...
====

. Verify the drive is active in cluster CR and node.
+
====
 kubectl describe adscl -n astrads-system
====
+
====
 ...
 Status:              Added
    Drive Statuses:
      Drive ID:       d6a4383b-305f-54d9-8264-990ff2964c15
      Drive Name:     scsi-36000c2949046697ae1c738208ffc6620
      Drive Serial:   6000c2949046697ae1c738208ffc6620
      Drives Status:  Available
      Drive ID:       55389866-fb73-57fd-9db8-96d5c78ea650
      Drive Name:     scsi-36000c29e16433c39e4d888b1dbbab6cf
      Drive Serial:   6000c29e16433c39e4d888b1dbbab6cf
      Drives Status:  Active
      Drive ID:       fc9b555d-0752-5497-ac79-a6e79d9a9ad0
      Drive Name:     scsi-36000c29fdafda4ab8852cc636c86b3c4
      Drive Serial:   6000c29fdafda4ab8852cc636c86b3c4
      Drives Status:  Active
      Drive ID:       a8bfd69b-c234-508b-882a-947508416d4f
      Drive Name:     scsi-36000c29339215b755d777ae20593e23b
      Drive Serial:   6000c29339215b755d777ae20593e23b
      Drives Status:  Active
    Maintenance Status:
      State:             Disabled
      Variant:           None
    Node HA:             true
    Node ID:             4
    Node Is Reachable:   true
    Node Management IP:  10.224.8.75
    Node Name:           sti-b200-0214b.ctl.gdl.englab.netapp.com
    Node Role:           Storage
    Node UUID:           29998974-a619-5269-86e2-f2aaaaaae107
    Node Version:        12.75.0.6169843
    Status:              Added
...
====


== Replace a node


. List the pods in the `astrads-system` namespace (our example uses a 1x5 configuration with 4 nodes in the cluster):
+
====
 kubectl get pods -n astrads-system
NAME                                 READY  STATUS    RESTARTS   AGE
astrads-cluster-controller...        1/1    Running   1          20h
astrads-deployment-support...        3/3    Running   0          20h
astrads-ds-cluster-multinodes-21209.  /1    Running   0          20h
====



. List all the nodes:
+
====
 kubectl astrads nodes list
NODE NAME           NODE STATUS    CLUSTER NAME
sti-rx2540-534d..   Added       cluster-multinodes-21209
sti-rx2540-535d...  Added       cluster-multinodes-21209
...
====

. Describe the cluster:
+
====
 kubectl astrads clusters list
CLUSTER NAME               CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209   created         4
====

. Verify that the Node HA is marked as "False" on the failed node:
+
====
 kubectl describe astradscluster -n astrads-system
Name:         cluster-multinodes-21209
Namespace:    astrads-system
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"astrads.netapp.io/v1alpha1","kind":"AstraDSCluster","metadata":{"annotations":{},"name":"cluster-multinodes-21209","namespa...
API Version:  astrads.netapp.io/v1alpha1
Kind:         AstraDSCluster
Metadata:
  Creation Timestamp:  2021-10-19T09:12:03Z
  Finalizers:
    astrads.netapp.io/astradscluster-finalizer
  Generation:  1
  Managed Fields:
    API Version:  astrads.netapp.io/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
        ...

    Manager:      autosupport-controller
    Operation:    Update
    Time:         2021-10-19T09:12:36Z
    API Version:  astrads.netapp.io/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:finalizers:
          ...

    Manager:      operator
    Operation:    Update
    Time:         2021-10-19T09:13:18Z
    API Version:  astrads.netapp.io/v1alpha1
    Fields Type:  FieldsV1

    Manager:      cluster-controller
    Operation:    Update
    Time:         2021-10-20T09:46:31Z
    API Version:  astrads.netapp.io/v1alpha1
    Fields Type:  FieldsV1

    Manager:         license-controller
    Operation:       Update
    Time:            2021-10-20T09:46:52Z
  Resource Version:  217898
  UID:               97ae6f6f-004d-4490-8a90-2dcdc01b9d8f
Spec:
  Ads Data Networks:
    Addresses:  ...
    Netmask:    255.255.252.0
  Ads Network Interfaces:
    Cluster Interface:     data
    Management Interface:  mgmt
    Storage Interface:     data
  Ads Node Config:
    Capacity:       600
    Cpu:            9
    Drives Filter:  .*
    Memory:         34
  Ads Node Count:   4
  Auto Support Config:
    Auto Upload:              true
    Coredump Upload:          false
    Destination URL:          ...
    Enabled:                  true
    History Retention Count:  25
    Periodic:
      Periodicconfig:
        Component:
          Event:           dailyMonitoring
          Name:            storage
        Force Upload:      false
        Local Collection:  false
        Nodes:             all
        Priority:          notice
        Retry:             false
        User Message:      Daily Monitoring Storage AutoSupport bundle
        Component:
          Event:           daily
          Name:            controlplane
        Force Upload:      false
        Local Collection:  false
        Priority:          notice
        Retry:             false
        User Message:      Daily Control Plane AutoSupport bundle
      Schedule:            0 0 * * *
  Monitoring Config:
    Namespace:  netapp-monitoring
    Repo:       docker.repo.eng.netapp.com/global/astra
  Mvip:         172.21.192.236
Status:
  Ads Data Addresses:
    Address:       ...
    Current Node:  1
    Uuid:          ...
...
  Autosupport:
    Periodicmap:
      Autosupport - Zieyo:
        Periodicconfig:
          Component:
            Event:           dailyMonitoring
            Name:            storage
          Force Upload:      false
          Local Collection:  false
          Nodes:             all
          Priority:          notice
          Retry:             false
          User Message:      Daily Monitoring Storage AutoSupport bundle
          Component:
            Event:           daily
            Name:            controlplane
          Force Upload:      false
          Local Collection:  false
          Priority:          notice
          Retry:             false
          User Message:      Daily Control Plane AutoSupport bundle
        Schedule:            0 0 * * *
  Cluster Status:            created
  Cluster UUID:              cd7c9a27-74b2-4c74-b565-cb816fe55fdd
  Conditions:
    Last Transition Time:  2021-10-19T09:12:04Z
    Last Update Time:      2021-10-19T09:12:04Z
    Message:               ADS Cluster configured properly for license
    Reason:                LicenseNormal
    Status:                False
    Type:                  LicenseExceeded
    Last Transition Time:  2021-10-19T09:12:10Z
    Last Update Time:      2021-10-20T09:46:52Z
    Message:               License has no restrictions present
    Status:                False
    Type:                  RestrictedLicense
    Last Transition Time:  2021-10-19T09:12:10Z
    Last Update Time:      2021-10-20T09:46:52Z
    Message:               License Valid
    Status:                True
    Type:                  ValidLicense
    Last Transition Time:  2021-10-19T09:12:10Z
    Last Update Time:      2021-10-20T09:46:52Z
    Status:                True
    Type:                  LastLicenseTransitionAttemptSuccessful
    Last Transition Time:  2021-10-20T09:27:35Z
    Last Update Time:      2021-10-20T09:45:32Z
    Message:               Firetap Cluster is unhealthy
    Reason:                ClusterFaults
    Status:                False
    Type:                  FiretapClusterHealthy
  Desired Versions:
    Ads:      2021.10.0
    Firetap:  12.75.0.6167444
  Ft Cluster Health:
    Details:
      Cluster Faults:
        Code:             NodeOffline
        Details:          The Distributed Block Store Application cannot communicate with Storage node having node ID 4.
        Node Id:          4
        Timestamp:        2021-10-20T09:26:43Z
        Code:             UnresponsiveService
        Details:          A master service is not responding.
        Node Id:          4
        Timestamp:        2021-10-20T09:28:06Z
        Code:             UnresponsiveService
        Details:          A firefly service is not responding.
        Node Id:          4
        Timestamp:        2021-10-20T09:28:11Z
      Syncing:            false
    Healthy:              false
  Ft Node Count:          4
  License Serial Number:  d900000011
  Node Statuses:
    Maintenance Status:
      State:             Disabled
      Variant:           None
    Node HA:             true
    Node ID:             1
    Node Is Reachable:   true
    Node Management IP:  172.21.192.251
    Node Name:           sti-rx2540-534d.ctl.gdl.englab.netapp.com
    Node Role:           Storage
    Node UUID:           f0f6d1af-cc71-5613-a4dd-d24456feafaa
    Node Version:        12.75.0.6167444
    Status:              Added
 ...

  Resources:
    Capacity Deployed:  2400
    Cpu Deployed:       36
  Versions:
    Ads:      2021.10.0
    Firetap:  12.75.0.6167444
Events:
  Type     Reason                      Age                     From         Message
  ----     ------                      ----                    ----         -------
  Warning  MonitoringConfigSetupError  4m32s (x7390 over 24h)  ADSOperator  Unable to setup monitoring agent for ADS cluster: monitoring CRD not found
====

. Modify the Cluster CR to remove the failed node. The node count decrements to 3:
+
====
 # rvi nate_hosts/netappsdscluster.yaml
 # cat nate_hosts/netappsdscluster.yaml t
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  name: cluster-multinodes-21209
  namespace: astrads-system
spec:
  # ADS Node Configuration per node settings
  adsNodeConfig:
    # Specify CPU limit for ADS components
    # Supported value: 9
    cpu: 9
    # Specify Memory Limit in GiB for ADS Components.
    # Your kubernetes worker nodes need to have at least this much RAM free
    # for ADS to function correctly
    # Supported value: 34
    memory: 34
    # [Optional] Specify raw storage consumption limit. The operator will only select drives for a node up to this limit
    capacity: 600
    # [Optional] Set a cache device if you do not want auto detection e.g. /dev/sdb
    # cacheDevice: ""
    # Set this regex filter to select drives for ADS cluster
    # drivesFilter: ".*"

  # [Optional] Specify node selector labels to select the nodes for creating ADS cluster
  # adsNodeSelector:
  #   matchLabels:
  #     customLabelKey: customLabelValue

  # Specify the number of nodes that should be used for creating ADS cluster
  adsNodeCount: 3

  # Specify the IP address of a floating management IP routable from any worker node in the cluster
  mvip: "172..."

  # Comma separated list of floating IP addresses routable from any host where you intend to mount a NetApp Volume
  # at least one per node must be specified
  # addresses: 10.0.0.1,10.0.0.2,10.0.0.3,10.0.0.4,10.0.0.5
  # netmask: 255.255.255.0
  adsDataNetworks:
    - addresses: "172..."
      netmask: 255.255.252.0

  # [Optional] Specify the network interface names for either all or none
  adsNetworkInterfaces:
    managementInterface: "mgmt"
    clusterInterface: "data"
    storageInterface: "data"

  # [Optional] Provide a k8s label key that defines which protection domain a node belongs to
  # adsProtectionDomainKey: ""

  # [Optional] Provide a monitoring config to be used to setup/configure a monitoring agent.
  monitoringConfig:
   namespace: "netapp-monitoring"
   repo: "docker.repo.eng.netapp.com/global/astra"

  autoSupportConfig:
    # AutoUpload defines the flag to enable or disable AutoSupport upload in the cluster (true/false)
    autoUpload: true
    # Enabled defines the flag to enable or disable automatic AutoSupport collection.
    # When set to false, periodic and event driven AutoSupport collection would be disabled.
    # It is still possible to trigger an AutoSupport manually while AutoSupport is disabled
    # enabled: true
    # CoredumpUpload defines the flag to enable or disable the upload of coredumps for this ADS Cluster
    # coredumpUpload: false
    # HistoryRetentionCount defines the number of local (not uploaded) AutoSupport Custom Resources to retain in the cluster before deletion
    historyRetentionCount: 25
    # DestinationURL defines the endpoint to transfer the AutoSupport bundle collection
    destinationURL: "https://testbed.netapp.com/put/AsupPut"
    # ProxyURL defines the URL of the proxy with port to be used for AutoSupport bundle transfer
    # proxyURL:
    # Periodic defines the config for periodic/scheduled AutoSupport objects
    periodic:
      # Schedule defines the Kubernetes Cronjob schedule
      - schedule: "0 0 * * *"
        # PeriodicConfig defines the fields needed to create the Periodic AutoSupports
        periodicconfig:
        - component:
            name: storage
            event: dailyMonitoring
          userMessage: Daily Monitoring Storage AutoSupport bundle
          nodes: all
        - component:
            name: controlplane
            event: daily
          userMessage: Daily Control Plane AutoSupport bundle
cat: t: No such file or directory
[root@scspr2409016001 42733317_42952507_1x5Node_Astra_DAS-002]# cat nate_hosts/netappsdscluster.yaml
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  name: cluster-multinodes-21209
  namespace: astrads-system
spec:
  # ADS Node Configuration per node settings
  adsNodeConfig:
    # Specify CPU limit for ADS components
    # Supported value: 9
    cpu: 9
    # Specify Memory Limit in GiB for ADS Components.
    # Your kubernetes worker nodes need to have at least this much RAM free
    # for ADS to function correctly
    # Supported value: 34
    memory: 34
    # [Optional] Specify raw storage consumption limit. The operator will only select drives for a node up to this limit
    capacity: 600
    # [Optional] Set a cache device if you do not want auto detection e.g. /dev/sdb
    # cacheDevice: ""
    # Set this regex filter to select drives for ADS cluster
    # drivesFilter: ".*"

  # [Optional] Specify node selector labels to select the nodes for creating ADS cluster
  # adsNodeSelector:
  #   matchLabels:
  #     customLabelKey: customLabelValue

  # Specify the number of nodes that should be used for creating ADS cluster
  adsNodeCount: 3

  # Specify the IP address of a floating management IP routable from any worker node in the cluster
  mvip: "172..."

  # Comma separated list of floating IP addresses routable from any host where you intend to mount a NetApp Volume
  # at least one per node must be specified
  # addresses: 10.0.0.1,10.0.0.2,10.0.0.3,10.0.0.4,10.0.0.5
  # netmask: 255.255.255.0
  adsDataNetworks:
    - addresses: "172..."
      netmask: 255.255.252.0

  # [Optional] Specify the network interface names for either all or none
  adsNetworkInterfaces:
    managementInterface: "mgmt"
    clusterInterface: "data"
    storageInterface: "data"

  # [Optional] Provide a k8s label key that defines which protection domain a node belongs to
  # adsProtectionDomainKey: ""

  # [Optional] Provide a monitoring config to be used to setup/configure a monitoring agent.
  monitoringConfig:
   namespace: "netapp-monitoring"
   repo: "docker.repo.eng.netapp.com/global/astra"

  autoSupportConfig:
    # AutoUpload defines the flag to enable or disable AutoSupport upload in the cluster (true/false)
    autoUpload: true
    # Enabled defines the flag to enable or disable automatic AutoSupport collection.
    # When set to false, periodic and event driven AutoSupport collection would be disabled.
    # It is still possible to trigger an AutoSupport manually while AutoSupport is disabled
    # enabled: true
    # CoredumpUpload defines the flag to enable or disable the upload of coredumps for this ADS Cluster
    # coredumpUpload: false
    # HistoryRetentionCount defines the number of local (not uploaded) AutoSupport Custom Resources to retain in the cluster before deletion
    historyRetentionCount: 25
    # DestinationURL defines the endpoint to transfer the AutoSupport bundle collection
    destinationURL: "https://testbed.netapp.com/put/AsupPut"
    # ProxyURL defines the URL of the proxy with port to be used for AutoSupport bundle transfer
    # proxyURL:

    # Periodic defines the config for periodic/scheduled AutoSupport objects
    periodic:
      # Schedule defines the Kubernetes Cronjob schedule
      - schedule: "0 0 * * *"
        # PeriodicConfig defines the fields needed to create the Periodic AutoSupports
        periodicconfig:
        - component:
            name: storage
            event: dailyMonitoring
          userMessage: Daily Monitoring Storage AutoSupport bundle
          nodes: all
        - component:
            name: controlplane
            event: daily
          userMessage: Daily Control Plane AutoSupport bundle
 kubectl apply -f nate_hosts/netappsdscluster.yaml
astradscluster.astrads.netapp.io/cluster-multinodes-21209 configured
====

. Verify the node is removed from the cluster:
+
====
 kubectl get nodes --show-labels
NAME                                            STATUS   ROLES                 AGE   VERSION   LABELS
sti-astramaster-237   Ready control-plane,master   24h   v1.20.0
sti-rx2540-532d       Ready  <none>                24h   v1.20.0
sti-rx2540-533d       Ready  <none>                24h

 kubectl get nodes --show-labels
NAME                  STATUS   ROLES                  AGE   VERSION   LABELS
sti-astramaster-237 Ready    control-plane,master   24h
sti-rx2540-532d     Ready    <none>                 24h

 kubectl astrads nodes list
NODE NAME         NODE STATUS     CLUSTER NAME
sti-rx2540-534d   Added           cluster-multinodes-21209
sti-rx2540-535d   Added           cluster-multinodes-21209
sti-rx2540-536d   Added           cluster-multinodes-21209

 kubectl astrads clusters list
CLUSTER NAME              CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209  created         3

 kubectl astrads drives list
DRIVE NAME   DRIVE ID    DRIVE STATUS  NODE NAME    CLUSTER NAME
scsi-36000c  c3e197f2... Active        rx2540...    cluster-multinodes-21209

 kubectl describe astradscluster -n astrads-system
Name:         cluster-multinodes-21209
Namespace:    astrads-system
Labels:       <none>
Kind:         AstraDSCluster
Metadata:
...
====

. Add a node to the cluster for replacement by modifying the cluster CR. The node count increments to 4. Verify that new node is picked up for addition.
+
====
 rvi nate_hosts/netappsdscluster.yaml
 cat nate_hosts/netappsdscluster.yaml
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  name: cluster-multinodes-21209
  namespace: astrads-system
====
+
====
 kubectl apply -f nate_hosts/netappsdscluster.yaml
astradscluster.astrads.netapp.io/cluster-multinodes-21209 configured

 kubectl get pods -n astrads-system
NAME                                READY   STATUS    RESTARTS   AGE
astrads-cluster-controller...       1/1     Running   1          24h
astrads-deployment-support...       3/3     Running   0          24h
astrads-ds-cluster-multinodes-21209 1/1     Running

 kubectl astrads nodes list
NODE NAME                NODE STATUS     CLUSTER NAME
sti-rx2540-534d...       Added           cluster-multinodes-21209
sti-rx2540-535d...       Added           cluster-multinodes-21209

 kubectl astrads clusters list
CLUSTER NAME                    CLUSTER STATUS  NODE COUNT
cluster-multinodes-21209        created         4

 kubectl astrads drives list
DRIVE NAME    DRIVE ID    DRIVE STATUS   NODE NAME     CLUSTER NAME
scsi-36000..  c3e197f2... Active         sti-rx2540... cluster-multinodes-21209
====
