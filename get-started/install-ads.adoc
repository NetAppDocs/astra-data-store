---
sidebar: sidebar
permalink: get-started/install-ads.html
keywords: astra, astra data store, install, deploy, download
summary: To install Astra Data Store after addressing some environmental prerequisites, you'll download the bundle and install following the steps described.
---

= Install Astra Data Store
:hardbreaks:
:icons: font
:imagesdir: ../media/get-started/

To install Astra Data Store, download the installation bundle from the NetApp Support Site and perform a series a commands to install Astra Data Store Operator and Astra Data Store in your environment. You can use this procedure to install Astra Data Store in internet-connected or air-gapped environments.

.What you'll need
* link:requirements.html[Before you begin installation, prepare your environment for Astra Data Store deployment].
* Root administrative permissions.
* If you are installing from an air-gapped environment, download the Astra Data Store bundle from the NetApp Support Site.

.About this task
The Astra Data Store installation process guides you through the following high-level steps:

* <<Download the Astra Data Store bundle>>
* <<Unpack the bundle and change directory>>
* <<Copy the binary and push images to your local registry>>
* <<Install the monitoring operator>>
* <<Install the Astra Data Store operator>>
* <<Edit and apply the Astra Data Store Deployment YAML>>
* <<Edit and apply the Astra Data Store License>>
* <<Install the Astra Data Store cluster>>

== Download the Astra Data Store bundle
. Log in to the NetApp Support Site and download the Astra Data Store bundle (`astra-data-store-[version].tar.gz`) from the https://mysupport.netapp.com/site/products/all/details/astra-data-store-downloads-tab[NetApp Support Site^].
. Download the zip of Astra Data Store license file (NLF) from https://mysupport.netapp.com/site/products/all/details/astra-data-store/downloads-tab[NetApp Support Site^].
. (Optional) Use the following command to verify the signature of the bundle:
+
----
openssl dgst -sha256 -verify astra-data-store[version].pub -signature <astra-data-store[version].sig astra-control-center[version].tar.gz
----

== Unpack the bundle and change directory

. Extract the images:
+
----
tar -vxzf astra-data-store-[version].tar.gz
----

. Change to the Astra directory.
+
----
cd astra-data-store-[version]
----

. Define the path where kubectl binary is to be installed. For example, `/usr/bin/kubectl` could be the binary path where kubectl is installed.
+
----
which kubectl
<install_path>
----

== Copy the binary and push images to your local registry

. Copy the kubectl-astrads binary to the standard path where k8s kubectl binaries are installed; for example, `/usr/bin/`. kubectl-astrads is a custom kubectl extension that installs and manages Astra DS clusters.
+
----
cp -f kubectl-astrads <path from previous command>
----

. Add the files in the Astra Data Store image directory to your local registry.
+
NOTE: See a sample script for the automatic loading of images below.

.. Log in to your Docker registry:
+
----
docker login [Docker_registry_path]
----

.. Load the images into Docker.
.. Tag the images.
.. Push the images to your local registry.

+
----
export REGISTRY=[Docker_registry_path]
for astraImageFile in $(ls images/*.tar)
  # Load to local cache. And store the name of the loaded image trimming the 'Loaded images: '
  do astraImage=$(docker load --input ${astraImageFile} | sed 's/Loaded image(s): //')
  astraImage=$(echo ${astraImage} | sed 's!localhost/!!')
  # Tag with local image repo.
  docker tag ${astraImage} ${REGISTRY}/${astraImage}
  # Push to the local repo.
  docker push ${REGISTRY}/${astraImage}
done
----

== Install the monitoring operator
(Optional) The monitoring operator is only recommended if Astra Data Store monitoring will not be performed in Astra Control Center. You can install the monitoring operator if you Astra Data Store instance is a standalone deployment, uses CI to monitor telemetry, or streams logs to a third-party endpoint such as Elastic.

. Run the install command:
----
[root@example ~]# helm3 install /u/samip/p4/netapp-monitoring --generate-name -n default
----

== Install the Astra Data Store operator
. List the AstraDS manifests:
+
----
[root@example ~]# ls manifests/
----
+
Response:
+
----
astradscluster.yaml
astradsdeployment.yaml
astradsoperator.yaml
----

. Edit the Astra Data Store operator custom resource definition (CRD) yaml to refer to your local registry and secret.
+
----
vim astradsoperator.yaml
----

.. Change `[Docker_registry_path]` for the `kube-rbac-proxy` image to the registry path where you pushed the images in a previous step.
.. Change `[Docker_registry_path]` for the `ads-operator` image to the registry path where you pushed the images in a previous step.
+
[subs=+quotes]
----
apiVersion: v1
kind: Namespace
metadata:
  labels:
    control-plane: operator
  name: astrads-system
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
.
.
.
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    control-plane: operator
  name: astrads-operator
  namespace: astrads-system
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: operator
  template:
    metadata:
      labels:
        control-plane: operator
    spec:
      containers:
      - args:
        - --secure-listen-address=0.0.0.0:8443
        - --upstream=http://127.0.0.1:8080/
        - --logtostderr=true
        - --v=10
        *image: [Docker_registry_path]/kube-rbac-proxy:v0.6.0*
        name: kube-rbac-proxy
        ports:
        - containerPort: 8443
          name: https
      - command:
        - /operator
        *image: [Docker_registry_path]/ads-operator:dev-6091923*
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - liveness -heartbeat 30
          failureThreshold: 3
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 15
        name: manager
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
      terminationGracePeriodSeconds: 10
----

. Apply the updated file to your Astra Data Store cluster:
+
----
kubectl apply -f astradsoperator.yaml
----

. Verify that the Astra Data Store operator pod has restarted and is running:
+
----
[root@example ~]$ kubectl get pods -n astrads-system
----
+
Response:
+
----
NAME                                READY   STATUS    RESTARTS   AGE
astrads-operator-56d9b69cf4-tkfcb   2/2     Running   0          85s
----

== Edit and apply the Astra Data Store Deployment YAML
. Edit the Astra Data Store deployment custom resource (CR) file:
.. VIM the yaml file:
+
----
vim astradsdeployment.yaml
----

.. Change `[Docker_registry_path]` to the registry path where you pushed the images in the previous step.
+
[subs=+quotes]
----
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSDeployment
metadata:
name: astradsdeployment
namespace: astrads-system
spec:
images:
*dmsController: [Docker_registry_path]/ads-dms-controller:dev-6093843*
*firetapInstaller: [Docker_registry_path]/ads-firetap-installer:dev-12.75.0-6091923*
*firegen: [Docker_registry_path]/ads-firetap-firegen:dev-6093843*
*firetapMetrics: [Docker_registry_path]/ads-firetap-metrics:dev-6091923*
*clusterController: [Docker_registry_path]/ads-cluster-controller:dev-6093843*
*support: [Docker_registry_path]/ads-support-controller:1.0*
*licenseController: [Docker_registry_path]/ads-license-controller:dev-6091923*
*callhomeListener: [Docker_registry_path]/ads-callhome-listener:dev-6093843*
*autosupportCronjob: [Docker_registry_path]/ads-autosupport-cronjob:dev-6093843*
*fluentBit: [Docker_registry_path]/fluent-bit:1.6.8*
*nodeInfoController: [Docker_registry_path]/ads-nodeinfo-controller:dev-6093843*
*kubeRbacProxy: [Docker_registry_path]/kube-rbac-proxy:v0.6.0*
version: 0.0.1
----
. Apply the updated file:
+
----
kubectl apply -f astradsdeployment.yaml
----

== Edit and apply the Astra Data Store License

. Edit and apply the NetApp License File (NLF) that you obtained from the NetApp Support Site (NSS) to your Astra Data Store cluster:

.. Copy and paste the content of the NLF after `netappLicenseFile:`.
.. Enter the name of the cluster that you are going to deploy or have already deployed.
+
[subs=+quotes]
----
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSLicense
metadata:
  name: "e900000005"
  namespace: "astrads-system"
spec:
  *netappLicenseFile: <NLF-contents>*
  *adsClusterName: "<Astra-Data-Store-cluster-name>"*
----

.. Create the license file:
+
----
[root@example ~]$ kubectl apply -f <sample-license-yaml>
----
+
Response:
+
----
astradslicense.astrads.netapp.io/e900000005 created
----

.. Verify the changes:
+
----
[<root ID> ~]$ kubectl get astradslicense -A
----
+
Response:
+
----
NAMESPACE        NAME         ADSCLUSTER                      VALID   PRODUCT                       EVALUATION   ENDDATE      VALIDATED
astrads-system   e900000005   astrads-sti-c6220-09-10-11-12   true    Astra Data Store Enterprise   true         2021-12-01   2021-06-23T23:36:11Z
----

== Install the Astra Data Store cluster
. VIM the yaml file:
+
----
vim astradscluster.yaml
----

. Edit the following values in the YAML file. An example follows these steps:
.. In `metadata`, change the `name` string to the name of your cluster.
.. Update the following required values in `spec`:
... Change the `mvip` string to the IP address of a floating management IP that is routable from any worker node in the cluster.
... In `adsDataNetworks`, list floating IP addresses (`addresses`) that are routable from any host where you intend to mount a NetApp volume.
... In `adsDataNetworks`, specify the netmask used by the data network.
... In `astraOptions`, add the license number (`serialNumber`) from the NLF.
... In `adsNodeConfig`, enter the per-node CPU core count and memory limits for the FireTap container.
.. (Optional) The following values can be optionally modified otherwise the default value will be used:
... In `spec`, enter a limit to how many nodes can be in the deployment (`adsNodeCount`).
... In `spec`, enter a selector label that filters out nodes for the cluster (`adsNodeSelector`).
...  In `spec`, provide a key that defines which protection domain a node belongs to (`adsProtectionDomainKey`).
... In `adsNetworkInterfaces`, enter the management, cluster, and storage interfaces.
... In `adsNodeConfig`, enter the per-node capacity, name of cache device to be configured for the FireTap container, and drive regex filter to select disks.
+
[subs=+quotes]
----
apiVersion: astrads.netapp.io/v1alpha1
kind: AstraDSCluster
metadata:
  *name: <name of your cluster>*
  namespace: astrads-system
spec:
  *mvip: <management IP address>*
  adsNodeCount: <optional node limit>
  adsNodeSelector: <optional selector label for node filtering>
  adsProtectionDomainKey: <optional key that defines which protection domain a node belongs to>
  adsDataNetworks:
    - *addresses: <CSV list of floating IP addresses>*
      *netmask: <The netmask used by dataNetworks>*
      gateway:
  adsNetworkInterfaces:
    managementInterface: <Management interface>
    clusterInterface: <Cluster interface>
    storageInterface: <Storage interface>
  astraOptions:
    *serialNumber: <serial number from license file>*
  adsNodeConfig:
    *cpu: <per-node cpu core count>*
    *memory: <per node memory limit>*
    capacity: <optional limit for per-node raw storage consumption>
    cacheDevice: <optional name of device to be configured as cache device for FireTap container>
    drivesFilter: <optional regex filter to select disks>
  autoSupportConfig:
    historyRetentionCount: 10
    destinationURL: "https://testbed.netapp.com/put/AsupPut"
    periodic:
      - schedule: "0 0 * * 0"
        periodicconfig:
        - component:
            name: controlplane
            event: weekly
          userMessage: Weekly Control Plane AutoSupport bundle
----

. Apply the updated file to your cluster:
+
----
kubectl apply -f astradscluster.yaml
----

. Verify the cluster deployment progress:
+
----
kubectl get astradscluster -n astrads-system
----
+
Sample return:
+
----
NAME                        STATUS    VERSION                            SERIAL NUMBER   MVIP           AGE

sample-0309d8b   created   sample-9.11.0-6090501   081856669       10.224.8.232   13d
----

. Run the following bash script after cluster creation to reserve node CPU and memory resources to constrain k8s:
//Confirm still needed???
+
----
#!/bin/bash
set -eio pipefail
CPU=8
MEM=32


CLUSTER_KIND="AstraDSCluster"
LDIR="/tmp/ADS"
LABEL_PREFIX="astrads.netapp.io"
SSH="ssh"
SCP="scp"
mkdir -p ${LDIR}
if ! CLUSTER_NAME=`kubectl get ${CLUSTER_KIND} -A -o jsonpath={.items[0].metadata.name}` ; then
        CLUSTER_NAME=""
fi
SCRIPT=${LDIR}/sys_res.sh
KUBE_RESERVED='{cpu: 8000m, memory: 32G}'
echo "#!/bin/bash
cat /var/lib/kubelet/config.yaml | python3 -c \"import yaml,sys; y = yaml.load(sys.stdin); y['systemReserved'] = yaml.safe_load(sys.argv[1]); print(yaml.dump(y,default_flow_style=False))\" \"${KUBE_RESERVED}\" > /var/lib/kubelet/config.yaml.new
mv /var/lib/kubelet/config.yaml.new /var/lib/kubelet/config.yaml
echo \"Restarting kubelet\"
systemctl restart kubelet
sleep 10
systemctl status kubelet
grep -A 3 "systemReserved" /var/lib/kubelet/config.yaml
" > ${SCRIPT}
kubectl get nodes  -L ${LABEL_PREFIX}/cluster -o wide
NODES=`kubectl get nodes -L ${LABEL_PREFIX}/cluster | awk /${CLUSTER_NAME}/'{print $1}'`
for NODE in $NODES ; do
        echo "$NODE"
        $SCP ${SCRIPT} root@${NODE}:sys_res.sh
        $SSH root@${NODE} chmod +x sys_res.sh
        $SSH root@${NODE} ./sys_res.sh
done
----

== What's next

Complete the deployment by performing link:setup-ads.html[setup tasks].
